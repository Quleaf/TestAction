name: 'Setup Rclone'
description: 'Configures rclone for S3 operations using system module rclone/1.68.1. Supports downloading archives from S3 and uploading files to S3 with optimized settings.'
inputs:
  access_key_id:
    description: 'Access Key ID for S3'
    required: true
  secret_access_key:
    description: 'Secret Access Key for S3'
    required: true
  endpoint:
    description: 'S3 Endpoint URL'
    required: true
  bucket:
    description: 'S3 Bucket Name'
    required: true
  destination_path:
    description: 'Destination path in the S3 bucket'
    required: true
  # Download-specific inputs
  download_mode:
    description: 'Whether to download archive from S3 (true/false)'
    required: false
    default: 'false'
  dockerfile_name:
    description: 'Dockerfile name for download (e.g., ex1) - required when download_mode=true'
    required: false
  version:
    description: 'Version for download (e.g., 0.0.5) - required when download_mode=true'
    required: false
  load_to_podman:
    description: 'Whether to load the downloaded archive into podman'
    required: false
    default: 'false'
  # Upload-specific inputs
  upload_mode:
    description: 'Whether to upload file to S3 (true/false)'
    required: false
    default: 'false'
  upload_file:
    description: 'Local file path to upload - required when upload_mode=true'
    required: false
  upload_file_type:
    description: 'Type of file being uploaded (archive/sif) for output naming'
    required: false
    default: 'archive'
outputs:
  rclone_loaded:
    description: 'Indicates if rclone module was loaded successfully'
    value: ${{ steps.load_rclone.outputs.loaded }}
  # Download-specific outputs
  archive_downloaded:
    description: 'Indicates if archive was downloaded'
    value: ${{ steps.download.outputs.downloaded }}
  archive_name:
    description: 'Downloaded archive filename'
    value: ${{ steps.download.outputs.archive_name }}
  archive_path:
    description: 'Full local path to downloaded archive'
    value: ${{ steps.download.outputs.archive_path }}
  archive_source:
    description: 'S3 source path of the archive'
    value: ${{ steps.download.outputs.archive_source }}
  image_tag:
    description: 'Image tag if loaded to podman'
    value: ${{ steps.load.outputs.image_tag }}
  # Upload-specific outputs
  file_uploaded:
    description: 'Indicates if file was uploaded to S3'
    value: ${{ steps.upload.outputs.uploaded }}
  upload_bucket:
    description: 'S3 bucket where file was uploaded'
    value: ${{ steps.upload.outputs.s3_bucket }}
  upload_path:
    description: 'S3 path of the uploaded file'
    value: ${{ steps.upload.outputs.s3_path }}
  upload_size:
    description: 'Size of the uploaded file'
    value: ${{ steps.upload.outputs.file_size }}
runs:
  using: 'composite'
  steps:
    - name: Load rclone module
      id: load_rclone
      shell: bash
      run: |
        set -euo pipefail
        echo "Loading rclone module..."
        if module load rclone/1.68.1; then
          echo "✓ rclone module loaded successfully"
          echo "loaded=true" >> $GITHUB_OUTPUT
          # Verify rclone is available
          if command -v rclone >/dev/null 2>&1; then
            echo "✓ rclone command is available"
            rclone version
          else
            echo "✗ rclone command not found after module load"
            exit 1
          fi
        else
          echo "✗ Failed to load rclone module"
          exit 1
        fi

    - name: Configure rclone (local config)
      shell: bash
      run: |
        set -euo pipefail
        # Load rclone module
        module load rclone/1.68.1
        # Store configuration in project-local directory
        mkdir -p ./.rclone_config
        tee ./.rclone_config/rclone.conf > /dev/null <<EOF
        [pawsey0012]
        type = s3
        provider = Ceph
        endpoint = ${{ inputs.endpoint }}
        access_key_id = ${{ inputs.access_key_id }}
        secret_access_key = ${{ inputs.secret_access_key }}
        EOF
        # Export environment variable so rclone automatically picks up this config
        echo "RCLONE_CONFIG=$(pwd)/.rclone_config/rclone.conf" >> $GITHUB_ENV

    - name: Verify rclone Configuration
      shell: bash
      run: |
        set -euo pipefail
        # Load rclone module
        module load rclone/1.68.1
        echo "Verifying rclone configuration..."
        rclone config show

    - name: Download archive from S3
      id: download
      if: inputs.download_mode == 'true'
      shell: bash
      run: |
        set -euo pipefail
        # Load rclone module
        module load rclone/1.68.1
        
        # Validate required inputs for download mode
        if [ -z "${{ inputs.dockerfile_name }}" ] || [ -z "${{ inputs.version }}" ]; then
          echo "Error: dockerfile_name and version are required when download_mode=true"
          exit 1
        fi
        
        bucket="${{ inputs.bucket }}"
        archive_file="${{ inputs.dockerfile_name }}_${{ inputs.version }}.tar"
        
        echo "Downloading archive from S3..."
        echo "Bucket: $bucket"
        echo "Archive: $archive_file"
        
        # Download from S3
        if rclone copy pawsey0012:"$bucket/$archive_file" ./ --progress; then
          if [ -f "./$archive_file" ]; then
            file_size=$(ls -lh "./$archive_file" | awk '{print $5}')
            echo "✓ Archive downloaded successfully from S3"
            echo "  [SOURCE]: s3://$bucket/$archive_file"
            echo "  [LOCAL]: ./$archive_file"
            echo "  [SIZE]: $file_size"
            
            echo "downloaded=true" >> "$GITHUB_OUTPUT"
            echo "archive_name=$archive_file" >> "$GITHUB_OUTPUT"
            echo "archive_path=${PWD}/$archive_file" >> "$GITHUB_OUTPUT"
            echo "archive_source=s3://$bucket/$archive_file" >> "$GITHUB_OUTPUT"
          else
            echo "✗ Archive download failed - file not found locally"
            exit 1
          fi
        else
          echo "✗ Failed to download archive from S3"
          echo "  Tried: s3://$bucket/$archive_file"
          echo "Available files in bucket:"
          rclone lsf pawsey0012:"$bucket/" | head -10 || echo "  (Cannot list bucket contents)"
          exit 1
        fi

    - name: Load archive into podman
      id: load
      if: inputs.download_mode == 'true' && inputs.load_to_podman == 'true'
      shell: bash
      run: |
        set -euo pipefail
        archive_file="${{ steps.download.outputs.archive_name }}"
        dockerfile_name="${{ inputs.dockerfile_name }}"
        version="${{ inputs.version }}"
        
        echo "Loading image from archive: $archive_file"
        podman load -i "$archive_file"
        
        # Verify image was loaded
        if podman images | grep -q "${dockerfile_name}"; then
          echo "✓ Image loaded successfully: ${dockerfile_name}:${version}"
          podman images | grep "${dockerfile_name}"
          echo "image_tag=${dockerfile_name}:${version}" >> "$GITHUB_OUTPUT"
        else
          echo "✗ Failed to load image: ${dockerfile_name}:${version}"
          exit 1
        fi

    - name: Upload file to S3 storage
      id: upload
      if: inputs.upload_mode == 'true'
      shell: bash
      run: |
        set -euo pipefail
        # Load rclone module
        module load rclone/1.68.1
        
        # Validate required inputs for upload mode
        if [ -z "${{ inputs.upload_file }}" ]; then
          echo "Error: upload_file is required when upload_mode=true"
          exit 1
        fi
        
        upload_file="${{ inputs.upload_file }}"
        bucket="${{ inputs.bucket }}"
        file_type="${{ inputs.upload_file_type }}"
        
        # Verify file exists
        if [ ! -f "$upload_file" ]; then
          echo "Error: Upload file not found: $upload_file"
          exit 1
        fi
        
        echo "Uploading file to S3..."
        echo "File: $upload_file"
        echo "Bucket: $bucket"
        echo "Type: $file_type"
        
        # Calculate file size for optimization
        FILE_SIZE=$(wc -c < "$upload_file")
        echo "File size: $FILE_SIZE bytes"
        
        # Set rclone parameters based on file size
        if [ "$FILE_SIZE" -lt $((1024 * 1024 * 500)) ]; then
          # < 500MB
          S3_CHUNK_SIZE="16M"
          S3_UPLOAD_CONCURRENCY=4
          MULTI_THREAD_STREAMS=2
        elif [ "$FILE_SIZE" -lt $((1024 * 1024 * 5000)) ]; then
          # 500MB - 5GB
          S3_CHUNK_SIZE="64M"
          S3_UPLOAD_CONCURRENCY=8
          MULTI_THREAD_STREAMS=4
        else
          # > 5GB
          S3_CHUNK_SIZE="128M"
          S3_UPLOAD_CONCURRENCY=16
          MULTI_THREAD_STREAMS=8
        fi
        
        echo "Optimized settings for $file_type upload:"
        echo "  S3 chunk size: $S3_CHUNK_SIZE"
        echo "  Upload concurrency: $S3_UPLOAD_CONCURRENCY"
        echo "  Multi-thread streams: $MULTI_THREAD_STREAMS"
        
        # Upload to S3
        if rclone copy "$upload_file" pawsey0012:"$bucket/" \
          --multi-thread-streams=$MULTI_THREAD_STREAMS \
          --s3-chunk-size=$S3_CHUNK_SIZE \
          --s3-upload-concurrency=$S3_UPLOAD_CONCURRENCY \
          --progress; then
          
          # Get filename for verification
          filename=$(basename "$upload_file")
          
          # Verify upload
          if rclone lsf pawsey0012:"$bucket/" | grep -q "^$filename$"; then
            file_size_human=$(ls -lh "$upload_file" | awk '{print $5}')
            echo "✓ File successfully uploaded to S3"
            echo "  [BUCKET]: $bucket"
            echo "  [S3 PATH]: $filename"
            echo "  [SIZE]: $file_size_human"
            
            # Set outputs
            echo "uploaded=true" >> "$GITHUB_OUTPUT"
            echo "s3_bucket=$bucket" >> "$GITHUB_OUTPUT"
            echo "s3_path=$filename" >> "$GITHUB_OUTPUT"
            echo "file_size=$file_size_human" >> "$GITHUB_OUTPUT"
          else
            echo "✗ Failed to verify S3 upload"
            exit 1
          fi
        else
          echo "✗ Failed to upload file to S3"
          echo "Available files in bucket:"
          rclone lsf pawsey0012:"$bucket/" | head -10 || echo "  (Cannot list bucket contents)"
          exit 1
        fi
